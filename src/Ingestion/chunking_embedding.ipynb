{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e67d2d2",
   "metadata": {},
   "source": [
    "This cell processes all PDFs in the input folder and extracts structured markdown text using Docling. Key variables include:\n",
    "\n",
    "- `input_dir`: The folder containing source PDF files.\n",
    "- `output_dir`: The folder where chunked `.json` and `.md` outputs will be saved.\n",
    "- `base_name`: The name of the file (without extension), used to generate matching output filenames.\n",
    "\n",
    "Before processing, the script checks whether a file has already been chunked by looking for a `.json` file in the output directory with the same base name. If it exists, the PDF is skipped to avoid redundant processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4570e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agish\\Desktop\\RAG-Therapy\\OneDrive_1_5-18-2025\\Thera-RAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Found 1 PDFs to process.\n",
      "🎯 Target chunk size: 5000 characters\n",
      "🔄 Smart overlap: 500 characters (10.0%)\n",
      "⏩ Skipping curry_et_al-2018-british_journal_of_haematology.pdf (already processed)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "try:\n",
    "    from docling.datamodel.pipeline_options import PdfPipelineOptions, AcceleratorOptions, AcceleratorDevice\n",
    "except ImportError:\n",
    "    # Fallback for older versions\n",
    "    from docling.datamodel.pipeline_options import PdfPipelineOptions, AcceleratorOptions\n",
    "    from docling.datamodel.acceleration_options import AcceleratorDevice\n",
    "except ImportError:\n",
    "    # Fallback for even older versions or different structure\n",
    "    from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "    AcceleratorOptions = None\n",
    "    AcceleratorDevice = None\n",
    "\n",
    "class DoclingBookLoader:\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        self.file_path = file_path\n",
    "        \n",
    "        # Handle different Docling versions\n",
    "        if AcceleratorOptions and AcceleratorDevice:\n",
    "            accelerator_options = AcceleratorOptions(num_threads=8, device=AcceleratorDevice.AUTO)\n",
    "            pipeline_options = PdfPipelineOptions(\n",
    "                accelerator_options=accelerator_options,\n",
    "                do_ocr=True,\n",
    "                do_table_structure=True,\n",
    "            )\n",
    "        else:\n",
    "            # Simplified options for older versions\n",
    "            pipeline_options = PdfPipelineOptions(\n",
    "                do_ocr=True,\n",
    "                do_table_structure=True,\n",
    "            )\n",
    "        \n",
    "        if hasattr(pipeline_options, 'table_structure_options'):\n",
    "            pipeline_options.table_structure_options.do_cell_matching = True\n",
    "\n",
    "        self.converter = DocumentConverter(\n",
    "            format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
    "        )\n",
    "\n",
    "    def extract_text(self) -> str:\n",
    "        docling_doc = self.converter.convert(self.file_path).document\n",
    "        return docling_doc.export_to_markdown()\n",
    "\n",
    "def create_smart_chunks(text: str, metadata: dict, target_chunk_size=3000, overlap_size=300) -> List[LCDocument]:\n",
    "    \"\"\"\n",
    "    Create larger, semantically meaningful chunks with smart overlap.\n",
    "    Uses strategic overlap to preserve context at boundaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate overlap ratio (should be 10-15% of chunk size)\n",
    "    overlap_ratio = overlap_size / target_chunk_size\n",
    "    if overlap_ratio > 0.2:  # Cap at 20%\n",
    "        overlap_size = int(target_chunk_size * 0.2)\n",
    "        print(f\"⚠️  Capped overlap to 20% of chunk size: {overlap_size} chars\")\n",
    "    \n",
    "    # First, try to split by major sections (headers) for natural boundaries\n",
    "    major_sections = text.split('\\n# ')\n",
    "    if len(major_sections) == 1:\n",
    "        # Try secondary headers if no major ones\n",
    "        major_sections = text.split('\\n## ')\n",
    "    if len(major_sections) == 1:\n",
    "        # Try tertiary headers\n",
    "        major_sections = text.split('\\n### ')\n",
    "    \n",
    "    chunks = []\n",
    "    previous_chunk_end = \"\"\n",
    "    chunk_id = 1\n",
    "    \n",
    "    # If we have natural sections, use them\n",
    "    if len(major_sections) > 1:\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for i, section in enumerate(major_sections):\n",
    "            # Re-add the header marker (except for first section)\n",
    "            if i > 0:\n",
    "                if '\\n# ' in text:\n",
    "                    section = '# ' + section\n",
    "                elif '\\n## ' in text:\n",
    "                    section = '## ' + section\n",
    "                elif '\\n### ' in text:\n",
    "                    section = '### ' + section\n",
    "            \n",
    "            # If adding this section would exceed target size, save current chunk\n",
    "            if len(current_chunk) + len(section) > target_chunk_size and current_chunk:\n",
    "                # Add overlap from previous chunk if available\n",
    "                chunk_content = previous_chunk_end + current_chunk\n",
    "                chunks.append(create_chunk_document(chunk_content, metadata, chunk_id))\n",
    "                \n",
    "                # Prepare overlap for next chunk\n",
    "                previous_chunk_end = get_smart_overlap(current_chunk, overlap_size)\n",
    "                current_chunk = section\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk += (\"\\n\" if current_chunk else \"\") + section\n",
    "        \n",
    "        # Add the final chunk\n",
    "        if current_chunk.strip():\n",
    "            chunk_content = previous_chunk_end + current_chunk\n",
    "            chunks.append(create_chunk_document(chunk_content, metadata, chunk_id))\n",
    "    \n",
    "    else:\n",
    "        # Fall back to recursive splitting with smart overlap\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=target_chunk_size,\n",
    "            chunk_overlap=overlap_size,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"],\n",
    "            keep_separator=True\n",
    "        )\n",
    "        doc = LCDocument(page_content=text, metadata=metadata)\n",
    "        split_chunks = splitter.split_documents([doc])\n",
    "        \n",
    "        for i, chunk in enumerate(split_chunks, 1):\n",
    "            chunk.metadata.update({\n",
    "                \"chunk_id\": f\"chunk_{i:03d}\",\n",
    "                \"content_hash\": hashlib.md5(chunk.page_content.encode()).hexdigest()[:12],\n",
    "                \"chunk_size\": len(chunk.page_content),\n",
    "                \"word_count\": len(chunk.page_content.split()),\n",
    "                \"is_sub_chunk\": False,\n",
    "                \"chunking_method\": \"recursive_with_overlap\"\n",
    "            })\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    # If chunks are still too large after section-based splitting, split them further\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk.page_content) > target_chunk_size * 1.5:\n",
    "            # Use recursive splitter for oversized chunks\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=target_chunk_size,\n",
    "                chunk_overlap=overlap_size,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "                keep_separator=True\n",
    "            )\n",
    "            sub_chunks = splitter.split_documents([chunk])\n",
    "            \n",
    "            # Update chunk IDs for sub-chunks\n",
    "            for j, sub_chunk in enumerate(sub_chunks):\n",
    "                sub_chunk.metadata.update({\n",
    "                    \"chunk_id\": f\"{chunk.metadata['chunk_id']}.{j+1}\",\n",
    "                    \"parent_chunk\": chunk.metadata['chunk_id'],\n",
    "                    \"is_sub_chunk\": True,\n",
    "                    \"chunking_method\": \"hybrid_section_recursive\"\n",
    "                })\n",
    "                final_chunks.append(sub_chunk)\n",
    "        else:\n",
    "            chunk.metadata[\"chunking_method\"] = \"section_based\"\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "def get_smart_overlap(text: str, overlap_size: int) -> str:\n",
    "    \"\"\"\n",
    "    Extract smart overlap that ends at sentence boundaries.\n",
    "    \"\"\"\n",
    "    if len(text) <= overlap_size:\n",
    "        return text\n",
    "    \n",
    "    # Try to end at sentence boundary\n",
    "    overlap_text = text[-overlap_size:]\n",
    "    \n",
    "    # Find the last sentence ending\n",
    "    for separator in ['. ', '! ', '? ', '\\n\\n']:\n",
    "        last_sep = overlap_text.rfind(separator)\n",
    "        if last_sep > overlap_size * 0.5:  # Must be at least 50% of desired overlap\n",
    "            return overlap_text[last_sep + len(separator):]\n",
    "    \n",
    "    # If no good sentence boundary, try paragraph\n",
    "    last_para = overlap_text.rfind('\\n')\n",
    "    if last_para > overlap_size * 0.3:\n",
    "        return overlap_text[last_para + 1:]\n",
    "    \n",
    "    # Fall back to character boundary\n",
    "    return overlap_text\n",
    "\n",
    "def create_chunk_document(content: str, base_metadata: dict, chunk_id: int) -> LCDocument:\n",
    "    \"\"\"Create a document chunk with enhanced metadata.\"\"\"\n",
    "    \n",
    "    # Generate a unique hash for the content\n",
    "    content_hash = hashlib.md5(content.encode()).hexdigest()[:12]\n",
    "    \n",
    "    # Extract potential section title from beginning of chunk\n",
    "    lines = content.strip().split('\\n')\n",
    "    section_title = \"\"\n",
    "    for line in lines[:3]:  # Check first 3 lines\n",
    "        if line.startswith('#'):\n",
    "            section_title = line.strip('#').strip()\n",
    "            break\n",
    "    \n",
    "    # Enhanced metadata\n",
    "    enhanced_metadata = {\n",
    "        **base_metadata,\n",
    "        \"chunk_id\": f\"chunk_{chunk_id:03d}\",\n",
    "        \"content_hash\": content_hash,\n",
    "        \"chunk_size\": len(content),\n",
    "        \"word_count\": len(content.split()),\n",
    "        \"section_title\": section_title,\n",
    "        \"starts_with\": content[:100].replace('\\n', ' '),\n",
    "        \"is_sub_chunk\": False,\n",
    "        \"chunking_method\": \"section_based_with_overlap\"\n",
    "    }\n",
    "    \n",
    "    return LCDocument(page_content=content.strip(), metadata=enhanced_metadata)\n",
    "\n",
    "def analyze_chunks(chunks: List[LCDocument]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze chunk statistics for optimization.\"\"\"\n",
    "    if not chunks:\n",
    "        return {}\n",
    "    \n",
    "    sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "    word_counts = [chunk.metadata.get('word_count', 0) for chunk in chunks]\n",
    "    \n",
    "    return {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_chunk_size\": sum(sizes) / len(sizes),\n",
    "        \"min_chunk_size\": min(sizes),\n",
    "        \"max_chunk_size\": max(sizes),\n",
    "        \"avg_word_count\": sum(word_counts) / len(word_counts),\n",
    "        \"sections_with_titles\": sum(1 for chunk in chunks if chunk.metadata.get('section_title')),\n",
    "        \"total_characters\": sum(sizes),\n",
    "        \"total_words\": sum(word_counts)\n",
    "    }\n",
    "\n",
    "def save_outputs(base_name: str, output_dir: str, markdown: str, chunks: List[LCDocument]):\n",
    "    \"\"\"Save outputs with enhanced analytics.\"\"\"\n",
    "    json_path = os.path.join(output_dir, f\"{base_name}.json\")\n",
    "    md_path = os.path.join(output_dir, f\"{base_name}.md\")\n",
    "    stats_path = os.path.join(output_dir, f\"{base_name}_stats.json\")\n",
    "\n",
    "    # Save markdown\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f_md:\n",
    "        f_md.write(markdown)\n",
    "\n",
    "    # Save chunks\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f_json:\n",
    "        json.dump(\n",
    "            [{\"content\": c.page_content, \"metadata\": c.metadata} for c in chunks],\n",
    "            f_json,\n",
    "            indent=2,\n",
    "            ensure_ascii=False\n",
    "        )\n",
    "    \n",
    "    # Save analytics\n",
    "    stats = analyze_chunks(chunks)\n",
    "    with open(stats_path, \"w\", encoding=\"utf-8\") as f_stats:\n",
    "        json.dump(stats, f_stats, indent=2)\n",
    "    \n",
    "    print(f\"✅ Saved {base_name} - {stats['total_chunks']} chunks, avg size: {stats['avg_chunk_size']:.0f} chars\")\n",
    "\n",
    "def process_pdf_folder(input_folder: str, output_folder: str, chunk_size: int = 3000, overlap_size: int = 300):\n",
    "    \"\"\"Process PDFs with configurable chunk size and smart overlap.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith(\".pdf\")]\n",
    "    print(f\"📂 Found {len(pdf_files)} PDFs to process.\")\n",
    "    print(f\"🎯 Target chunk size: {chunk_size} characters\")\n",
    "    print(f\"🔄 Smart overlap: {overlap_size} characters ({overlap_size/chunk_size*100:.1f}%)\")\n",
    "\n",
    "    total_stats = {\n",
    "        \"files_processed\": 0,\n",
    "        \"total_chunks\": 0,\n",
    "        \"total_characters\": 0,\n",
    "        \"avg_chunk_size_across_files\": []\n",
    "    }\n",
    "\n",
    "    for filename in pdf_files:\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        json_output = os.path.join(output_folder, f\"{base_name}.json\")\n",
    "\n",
    "        if os.path.exists(json_output):\n",
    "            print(f\"⏩ Skipping {filename} (already processed)\")\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(input_folder, filename)\n",
    "        print(f\"\\n📄 Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            loader = DoclingBookLoader(pdf_path)\n",
    "            markdown = loader.extract_text()\n",
    "            metadata = {\n",
    "                \"source\": filename,\n",
    "                \"processing_timestamp\": str(os.path.getmtime(pdf_path)),\n",
    "                \"chunk_strategy\": \"smart_semantic_with_overlap\",\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"overlap_size\": overlap_size\n",
    "            }\n",
    "            \n",
    "            chunks = create_smart_chunks(markdown, metadata, chunk_size, overlap_size)\n",
    "            save_outputs(base_name, output_folder, markdown, chunks)\n",
    "            \n",
    "            # Update total stats\n",
    "            stats = analyze_chunks(chunks)\n",
    "            total_stats[\"files_processed\"] += 1\n",
    "            total_stats[\"total_chunks\"] += stats[\"total_chunks\"]\n",
    "            total_stats[\"total_characters\"] += stats[\"total_characters\"]\n",
    "            total_stats[\"avg_chunk_size_across_files\"].append(stats[\"avg_chunk_size\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    if total_stats[\"files_processed\"] > 0:\n",
    "        overall_avg = sum(total_stats[\"avg_chunk_size_across_files\"]) / len(total_stats[\"avg_chunk_size_across_files\"])\n",
    "        print(f\"\\n📊 Processing Summary:\")\n",
    "        print(f\"   Files processed: {total_stats['files_processed']}\")\n",
    "        print(f\"   Total chunks created: {total_stats['total_chunks']}\")\n",
    "        print(f\"   Average chunk size: {overall_avg:.0f} characters\")\n",
    "        print(f\"   Total content: {total_stats['total_characters']:,} characters\")\n",
    "        print(f\"   Overlap strategy: {overlap_size} chars ({overlap_size/chunk_size*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"pdfs/\"\n",
    "    output_dir = \"outputs/\"\n",
    "    \n",
    "    # Recommended overlap sizes:\n",
    "    # 200-300 chars ≈ 1-2 sentences (conservative, good for most cases)\n",
    "    # 400-500 chars ≈ 2-3 sentences (more context preservation)\n",
    "    # 10-15% of chunk_size is typically optimal\n",
    "    \n",
    "    chunk_size = 5000      # Main chunk size\n",
    "    overlap_size = 500     # Smart overlap (10% of chunk size)\n",
    "    \n",
    "    process_pdf_folder(input_dir, output_dir, chunk_size, overlap_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c463343",
   "metadata": {},
   "source": [
    "### Embedding & Uploading to Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb224bb3",
   "metadata": {},
   "source": [
    "This cell loads environment variables, initializes the Pinecone client, and uploads document embeddings to a Pinecone vector index:\n",
    "\n",
    "- It loads the `PINECONE_API_KEY` from a `.env` file and initializes a Pinecone index named `medical-rag-index` in the `us-east-1` AWS region.\n",
    "- It uses the `intfloat/e5-base` model from SentenceTransformers to embed each document chunk.\n",
    "- Chunks are read from JSON files in the `outputs/` folder, embedded, and uploaded in batches to the Pinecone index under the `thera-rag` namespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518086d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84adeeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Model loaded\n",
      "\n",
      "📁 Reading chunks from: outputs\n",
      "Found 1 JSON files to process\n",
      "\n",
      "🔍 Inspecting outputs\\curry_et_al-2018-british_journal_of_haematology.json:\n",
      "   Type: <class 'list'>\n",
      "   Length: 24\n",
      "   First entry type: <class 'dict'>\n",
      "   First entry keys: ['content', 'metadata']\n",
      "   Entry 1: {'content': '<!-- image -->\\n\\n## The use of viscoelastic haemostatic assays in the management of ma...\n",
      "   Entry 2: {'content': 'The Association of Anaesthetists of Great Britain and Ireland endorsed the document.\\n#...\n",
      "   Entry 3: {'content': \"<!-- image -->\\n\\n|                 | TEG 5000 (cup and pin method)         | TEG 6s (c...\n",
      "\n",
      "🧠 Embedding chunks from curry_et_al-2018-british_journal_of_haematology.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing curry_et_al-2018-british_journal_of_haematology.json: 100%|██████████| 24/24 [00:26<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬆️  Final batch: Uploaded 24 vectors.\n",
      "\n",
      "✅ Done! Processed 24 chunks total.\n",
      "📊 Pinecone index stats:\n",
      "   Total vectors: 24\n",
      "   Namespace 'thera-rag': 24\n",
      "\n",
      "🧪 Testing query...\n",
      "✅ Query test successful! Found 3 results for 'medical treatment'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load env variables\n",
    "load_dotenv()\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "assert pinecone_api_key, \"Missing Pinecone env vars in .env\"\n",
    "\n",
    "# ✅ Initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Settings\n",
    "index_name = \"medical-rag-index\"\n",
    "namespace = \"thera-rag\"\n",
    "embed_dim = 768  # e5-base\n",
    "\n",
    "# 🔧 Create index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    cloud = \"aws\"           \n",
    "    region = \"us-east-1\"     \n",
    "\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embed_dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=cloud, region=region)\n",
    "    )\n",
    "    print(f\"✅ Created index '{index_name}' with serverless backend in {cloud}/{region}\")\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Load model\n",
    "embedder = SentenceTransformer(\"intfloat/e5-base\")\n",
    "print(\"🔍 Model loaded\")\n",
    "\n",
    "def extract_chunk_data(entry):\n",
    "    \"\"\"\n",
    "    Robust function to extract content and metadata from different JSON structures.\n",
    "    \"\"\"\n",
    "    # Case 1: {\"content\": \"...\", \"metadata\": {...}}\n",
    "    if isinstance(entry, dict) and \"content\" in entry:\n",
    "        content = entry[\"content\"]\n",
    "        metadata = entry.get(\"metadata\", {})\n",
    "        return content, metadata\n",
    "    \n",
    "    # Case 2: {\"page_content\": \"...\", \"metadata\": {...}} (LangChain format)\n",
    "    elif isinstance(entry, dict) and \"page_content\" in entry:\n",
    "        content = entry[\"page_content\"]\n",
    "        metadata = entry.get(\"metadata\", {})\n",
    "        return content, metadata\n",
    "    \n",
    "    # Case 3: Just a string (raw content)\n",
    "    elif isinstance(entry, str):\n",
    "        return entry, {}\n",
    "    \n",
    "    # Case 4: Other dict format - try to find text content\n",
    "    elif isinstance(entry, dict):\n",
    "        # Look for common text fields\n",
    "        for field in [\"text\", \"content\", \"page_content\", \"body\", \"chunk\"]:\n",
    "            if field in entry:\n",
    "                content = entry[field]\n",
    "                metadata = {k: v for k, v in entry.items() if k != field}\n",
    "                return content, metadata\n",
    "    \n",
    "    # Fallback: convert to string\n",
    "    return str(entry), {}\n",
    "\n",
    "def inspect_json_structure(file_path, max_entries=3):\n",
    "    \"\"\"Debug function to inspect JSON structure.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n🔍 Inspecting {file_path}:\")\n",
    "    print(f\"   Type: {type(data)}\")\n",
    "    print(f\"   Length: {len(data) if isinstance(data, (list, dict)) else 'N/A'}\")\n",
    "    \n",
    "    if isinstance(data, list) and data:\n",
    "        print(f\"   First entry type: {type(data[0])}\")\n",
    "        print(f\"   First entry keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'Not a dict'}\")\n",
    "        \n",
    "        # Show sample entries\n",
    "        for i, entry in enumerate(data[:max_entries]):\n",
    "            print(f\"   Entry {i+1}: {str(entry)[:100]}...\")\n",
    "    elif isinstance(data, dict):\n",
    "        print(f\"   Dict keys: {list(data.keys())}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load chunks\n",
    "chunk_dir = \"outputs\"\n",
    "batch_size = 100\n",
    "vectors = []\n",
    "total_processed = 0\n",
    "\n",
    "print(f\"\\n📁 Reading chunks from: {chunk_dir}\")\n",
    "\n",
    "json_files = [f for f in os.listdir(chunk_dir) if f.endswith(\".json\") and not f.endswith(\"_stats.json\")]\n",
    "print(f\"Found {len(json_files)} JSON files to process\")\n",
    "\n",
    "for file in json_files:\n",
    "    if file.endswith(\"_stats.json\"):  # Skip stats files\n",
    "        continue\n",
    "        \n",
    "    file_path = os.path.join(chunk_dir, file)\n",
    "    \n",
    "    try:\n",
    "        # First, inspect the structure\n",
    "        chunks = inspect_json_structure(file_path)\n",
    "        \n",
    "        print(f\"\\n🧠 Embedding chunks from {file}...\")\n",
    "        \n",
    "        # Handle different data structures\n",
    "        if isinstance(chunks, list):\n",
    "            chunk_list = chunks\n",
    "        elif isinstance(chunks, dict):\n",
    "            # Maybe it's wrapped in a key\n",
    "            chunk_list = chunks.get('chunks', chunks.get('data', [chunks]))\n",
    "        else:\n",
    "            print(f\"⚠️ Unexpected data type in {file}: {type(chunks)}\")\n",
    "            continue\n",
    "\n",
    "        for i, entry in enumerate(tqdm(chunk_list, desc=f\"Processing {file}\")):\n",
    "            try:\n",
    "                content, metadata = extract_chunk_data(entry)\n",
    "                \n",
    "                if not content or not content.strip():\n",
    "                    continue\n",
    "\n",
    "                # Create a unique ID using file name and index\n",
    "                chunk_id = metadata.get(\"chunk_id\", f\"{file}_{i:03d}\")\n",
    "                unique_id = f\"{os.path.splitext(file)[0]}_{chunk_id}\"\n",
    "                \n",
    "                # Prepare text for embedding (E5 format)\n",
    "                input_text = f\"passage: {content.strip()}\"\n",
    "                embedding = embedder.encode(input_text, normalize_embeddings=True)\n",
    "\n",
    "                # Prepare metadata for Pinecone (ensure all values are strings/numbers)\n",
    "                pinecone_metadata = {\n",
    "                    \"text\": content[:500],  # Truncate for Pinecone metadata limits\n",
    "                    \"source_file\": file,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"chunk_size\": len(content),\n",
    "                    \"word_count\": len(content.split())\n",
    "                }\n",
    "                \n",
    "                # Add original metadata, ensuring compatibility\n",
    "                for key, value in metadata.items():\n",
    "                    if isinstance(value, (str, int, float, bool)):\n",
    "                        pinecone_metadata[key] = value\n",
    "                    else:\n",
    "                        pinecone_metadata[key] = str(value)\n",
    "\n",
    "                vectors.append({\n",
    "                    \"id\": unique_id,\n",
    "                    \"values\": embedding.tolist(),\n",
    "                    \"metadata\": pinecone_metadata\n",
    "                })\n",
    "\n",
    "                total_processed += 1\n",
    "\n",
    "                # Batch upload\n",
    "                if len(vectors) >= batch_size:\n",
    "                    index.upsert(vectors=vectors, namespace=namespace)\n",
    "                    print(f\"⬆️  Uploaded batch: {len(vectors)} vectors (Total: {total_processed})\")\n",
    "                    vectors = []\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing entry {i} in {file}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file {file}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Final upload\n",
    "if vectors:\n",
    "    index.upsert(vectors=vectors, namespace=namespace)\n",
    "    print(f\"⬆️  Final batch: Uploaded {len(vectors)} vectors.\")\n",
    "\n",
    "# Print summary\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\n✅ Done! Processed {total_processed} chunks total.\")\n",
    "print(f\"📊 Pinecone index stats:\")\n",
    "print(f\"   Total vectors: {stats.total_vector_count}\")\n",
    "print(f\"   Namespace '{namespace}': {stats.namespaces.get(namespace, {}).vector_count if stats.namespaces else 'Not found'}\")\n",
    "\n",
    "# Test a quick query to verify everything works\n",
    "if total_processed > 0:\n",
    "    print(f\"\\n🧪 Testing query...\")\n",
    "    test_query = \"medical treatment\"\n",
    "    test_embedding = embedder.encode(f\"query: {test_query}\", normalize_embeddings=True)\n",
    "    \n",
    "    results = index.query(\n",
    "        vector=test_embedding.tolist(),\n",
    "        top_k=3,\n",
    "        namespace=namespace,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Query test successful! Found {len(results.matches)} results for '{test_query}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
